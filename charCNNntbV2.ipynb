{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras_preprocessing.text import Tokenizer\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Input, Dense, Conv1D, MaxPool1D, Activation, Embedding, Flatten, BatchNormalization\n",
    "from tensorflow.keras.models import Model\n",
    "import os\n",
    "from functools import reduce\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'UNK': 1, 'c': 2, '=': 3, '(': 4, ')': 5, 'o': 6, 'n': 7, '1': 8, '2': 9, '@': 10, '3': 11, '[': 12, ']': 13, 'h': 14, '4': 15, '/': 16, 'f': 17, '5': 18, 's': 19, '\\\\': 20, 'l': 21, '6': 22, 'p': 23, '-': 24, '+': 25, '#': 26, '7': 27, 'b': 28, 'r': 29, '8': 30, '9': 31, 'i': 32, '%': 33, '0': 34}\n",
      "word index len:  34\n",
      "max:  1185\n",
      "sum  1370458\n",
      "avg_len:  60.60487330296732\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df = pd.read_csv(\"https://raw.githubusercontent.com/manuelr417/charecterlevel/master/sample_training2.csv\")\n",
    "texts = df.iloc[:,0].to_list()\n",
    "\n",
    "tk =  Tokenizer(num_words=None, char_level=True, oov_token='UNK')\n",
    "tk.fit_on_texts(texts)\n",
    "print(tk.word_index)\n",
    "print(\"word index len: \", len(tk.word_index))\n",
    "\n",
    "sequences = tk.texts_to_sequences(texts)\n",
    "#print(texts[0])\n",
    "#print(sequences[0])\n",
    "\n",
    "lens = [len(x) for i, x in enumerate(sequences)]\n",
    "#print(lens)\n",
    "print(\"max: \", max(lens))\n",
    "sum_ser = reduce(lambda x, y: x + y, lens)\n",
    "print(\"sum \", sum_ser)\n",
    "avg_len = (sum_ser * 1.0)/(len(lens))\n",
    "print(\"avg_len: \", avg_len)\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=1400, padding='post')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape X  (22613, 1400)\n",
      "Shape Y  (22613,)\n"
     ]
    }
   ],
   "source": [
    "np_data = np.array(data)\n",
    "print(\"Shape X \", np_data.shape)\n",
    "xlogs = df.iloc[:, 1].to_list()\n",
    "\n",
    "y_data = np.array(xlogs)\n",
    "\n",
    "print(\"Shape Y \", y_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Neural net\n",
    "input_size = 1400\n",
    "dimension = 50\n",
    "vocabulary_size = len(tk.word_index)\n",
    "\n",
    "input_layer = Input(shape=(input_size,), name=\"input_layer\")\n",
    "embedding_layer = Embedding(vocabulary_size + 1, dimension, input_length=input_size, name=\"embedding\")(input_layer)\n",
    "num_filters = 64\n",
    "filter_size = 7\n",
    "\n",
    "X = Conv1D(num_filters, filter_size, padding='same', name=\"conv1\")(embedding_layer)\n",
    "X = BatchNormalization()(X)\n",
    "X = Activation(activation='relu')(X)\n",
    "#X = MaxPool1D(pool_size=2, name=\"maxpool1\")(conv_1)\n",
    "\n",
    "num_filters = 64\n",
    "filter_size = 7\n",
    "\n",
    "\n",
    "X = Conv1D(num_filters, filter_size, padding='same', name=\"conv2\")(X)\n",
    "X = BatchNormalization()(X)\n",
    "X = Activation(activation='relu')(X)\n",
    "#max_pool2 = MaxPool1D(pool_size=2, name=\"maxpool2\")(conv_2)\n",
    "\n",
    "num_filters = 64\n",
    "filter_size = 7\n",
    "\n",
    "\n",
    "X = Conv1D(num_filters, filter_size, padding='same', name=\"conv3\")(X)\n",
    "X = BatchNormalization()(X)\n",
    "X = Activation(activation='relu')(X)\n",
    "#max_pool3 = MaxPool1D(pool_size=2, name=\"maxpool3\")(conv_3)\n",
    "\n",
    "\n",
    "X = Flatten()(X)\n",
    "X = Dense(1024*2, activation='relu', name=\"dense1\")(X)\n",
    "X = Dense(1024, activation='relu', name=\"dense2\")(X)\n",
    "X = Dense(512, activation='relu', name=\"dense3\")(X)\n",
    "X = Dense(512/2, activation='relu', name=\"dense4\")(X)\n",
    "\n",
    "output = Dense(1, name=\"dense5\")(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_layer (InputLayer)     [(None, 1400)]            0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, 1400, 50)          1750      \n",
      "_________________________________________________________________\n",
      "conv1 (Conv1D)               (None, 1400, 64)          22464     \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 1400, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 1400, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2 (Conv1D)               (None, 1400, 64)          28736     \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 1400, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 1400, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv3 (Conv1D)               (None, 1400, 64)          28736     \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 1400, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 1400, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 89600)             0         \n",
      "_________________________________________________________________\n",
      "dense1 (Dense)               (None, 2048)              183502848 \n",
      "_________________________________________________________________\n",
      "dense2 (Dense)               (None, 1024)              2098176   \n",
      "_________________________________________________________________\n",
      "dense3 (Dense)               (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "dense4 (Dense)               (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dense5 (Dense)               (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 186,339,863\n",
      "Trainable params: 186,339,479\n",
      "Non-trainable params: 384\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Model(inputs=input_layer, outputs=output)\n",
    "model.compile(optimizer='adam', loss='mae', metrics=['mse', 'mae']) # Adam, categorical_crossentropy\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "135/248 [===============>..............] - ETA: 2:27 - loss: 12.0412 - mse: 2444.6331 - mae: 12.0412"
     ]
    }
   ],
   "source": [
    "model.fit(np_data, y_data, epochs=10, batch_size= 64, validation_split=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}